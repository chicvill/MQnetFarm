import asyncio
import json
import csv
import os
import random
import sys
from datetime import datetime
from sf_core import ESP32C3Node, SYSTEM_REGISTRY, set_data_dir

# ğŸŸ¢ Google Sheets Support
try:
    import gspread
    from google.oauth2.service_account import Credentials
    GS_ENABLED = True
except ImportError:
    GS_ENABLED = False

# Add current directory to sys.path
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

# ğŸ“‚ ë°ì´í„° í´ë” ì„¤ì • (í™˜ê²½ ë³€ìˆ˜ ë˜ëŠ” ê¸°ë³¸ê°’)
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
DATA_DIR = os.environ.get('DATA_DIR', 'data').strip()

# ì˜¤íƒ€ ë°©ì§€ìš© ë³´ì •
if DATA_DIR == 'busan-data' and not os.path.exists(os.path.join(BASE_DIR, 'busan-data')) and os.path.exists(os.path.join(BASE_DIR, 'busan_data')):
    DATA_DIR = 'busan_data'

set_data_dir(DATA_DIR)

print(f"ğŸ”§ [System] BASE_DIR: {BASE_DIR}")
print(f"ğŸ“‚ [System] DATA_DIR: {DATA_DIR}")

# Vision Analysis (Optional)
try:
    import vision_analysis
    print("âœ… [Vision] Vision Module Loaded Successfully.")
except ImportError as e:
    print(f"âš ï¸ [Vision] Vision Module Load Failed: {e}")
    vision_analysis = None

# Google Sheets ì „ìš© ì „ì—­ ê°ì²´
GS_CLIENT = None
GS_SHEET = None

def init_google_sheets():
    global GS_CLIENT, GS_SHEET
    if not GS_ENABLED: return None
    
    sheet_name = os.environ.get('GS_SHEET_NAME', 'SmartFarm_Data')
    
    # Render Secret Files ë° ë¡œì»¬ ê²½ë¡œ íƒìƒ‰
    possible_paths = [
        os.environ.get('GS_CRED_PATH', ''), 
        'my_secret_key.json',           # íšŒí”¼ìš© ìƒˆ ì´ë¦„
        'credentials.json',
        '/etc/secrets/my_secret_key.json', # Renderìš© ìƒˆ ì´ë¦„
        '/etc/secrets/credentials.json'    # Render ê¸°ë³¸ ê²½ë¡œ
    ]
    
    cred_path = None
    for p in possible_paths:
        if p and os.path.exists(p):
            cred_path = p
            break
    
    if cred_path:
        try:
            scopes = ['https://www.googleapis.com/auth/spreadsheets', 'https://www.googleapis.com/auth/drive']
            creds = Credentials.from_service_account_file(cred_path, scopes=scopes)
            GS_CLIENT = gspread.authorize(creds)
            
            # ë¡œê·¸: ì ‘ì† ì‹œë„
            try:
                # 1. ì‹œíŠ¸ ì—´ê¸° ì‹œë„
                spreadsheet = GS_CLIENT.open(sheet_name)
                GS_SHEET = spreadsheet.get_worksheet(0)
                print(f"[Google] '{sheet_name}' ì—°ê²° ì„±ê³µ. (Path: {cred_path})")
                
                # [NEW] ë¹„ë™ê¸°ë¡œ ë¶€íŒ… ë¡œê·¸ ê¸°ë¡
                asyncio.create_task(async_update_gs([[datetime.now().strftime("%Y-%m-%d %H:%M:%S"), "SYSTEM", "BOOT", "Server Started", "OK", "0"]]))
                return True
            except gspread.exceptions.SpreadsheetNotFound:
                # 2. ëª» ì°¾ì•˜ì„ ê²½ìš°, ê¶Œí•œì´ ìˆëŠ” ì‹œíŠ¸ ëª©ë¡ ì¶œë ¥í•˜ì—¬ ê°€ì´ë“œ
                print(f"âš ï¸ [Google] '{sheet_name}' ì‹œíŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                titles = [s.title for s in GS_CLIENT.openall()]
                if titles:
                    print(f"   ã„´ í˜„ì¬ ì ‘ê·¼ ê°€ëŠ¥í•œ ì‹œíŠ¸: {titles}")
                else:
                    print(f"   ã„´ ì ‘ê·¼ ê°€ëŠ¥í•œ ì‹œíŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤. ê³µìœ  ì„¤ì •ì„ ë‹¤ì‹œ í™•ì¸í•˜ì„¸ìš” (Email: {creds.service_account_email})")
            except Exception as e:
                print(f"âš ï¸ [Google] ì‹œíŠ¸ ì ‘ê·¼ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
                
        except Exception as e:
            print(f"âš ï¸ [Google] ì‹œíŠ¸ ì¸ì¦ ì‹¤íŒ¨: {e}")
    else:
        # print("â„¹ï¸ [Google] credentials.json íŒŒì¼ì´ ì—†ì–´ ì‹œíŠ¸ ì—°ë™ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
        pass
    return False

# Google Sheets ë¹„ë™ê¸° ì—…ë°ì´íŠ¸ ë˜í¼
async def async_update_gs(rows):
    if not GS_SHEET: return
    try:
        # gspreadëŠ” ë™ê¸° ë°©ì‹ì´ë¯€ë¡œ ìŠ¤ë ˆë“œì—ì„œ ì‹¤í–‰í•˜ì—¬ ì´ë²¤íŠ¸ ë£¨í”„ ë°©í•´ ê¸ˆì§€
        await asyncio.to_thread(GS_SHEET.append_rows, rows)
        print(f"[Google] {len(rows)}ê±´ ì—…ë°ì´íŠ¸ ì™„ë£Œ.")
    except Exception as e:
        print(f"[Google] ì—…ë°ì´íŠ¸ ì—ëŸ¬: {e}")

# ì´ˆê¸°í™” í•¨ìˆ˜ ì •ì˜ (í˜¸ì¶œì€ mainì—ì„œ ìˆ˜í–‰)

def index_to_alpha(n):
    res = ""
    for _ in range(3):
        res = chr(65 + (n % 26)) + res
        n //= 26
    return res

async def tsdb_logger_task(interval=60):
    """
    ì£¼ê¸°ì ìœ¼ë¡œ ëª¨ë“  ì„¼ì„œ ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•˜ì—¬ CSV íŒŒì¼ì— ì‹œê³„ì—´ë¡œ ì €ì¥í•©ë‹ˆë‹¤.
    """
    file_path = f"{DATA_DIR}/smartfarm_tsdb.csv"
    
    # íŒŒì¼ì´ ì—†ìœ¼ë©´ í—¤ë” ìƒì„±
    if not os.path.exists(file_path):
        os.makedirs(os.path.dirname(file_path), exist_ok=True)
        with open(file_path, 'w', newline='', encoding='utf-8-sig') as f:
            writer = csv.writer(f)
            writer.writerow(["timestamp", "node_id", "device_id", "device_name", "value", "pin"])

    print(f"ğŸ“ˆ [TSDB] ì‹œê³„ì—´ ë¡œê¹… íƒœìŠ¤í¬ ê°€ë™ (ì£¼ê¸°: {interval}ì´ˆ)")
    
    # ì‹¤ì‹œê°„ ë°ì´í„° ê³µìœ ë¥¼ ìœ„í•œ íŒŒì¼ ê²½ë¡œ
    live_data_path = f"{DATA_DIR}/live_data.json"

    while True:
        try:
            timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
            log_entries = []
            live_status = {}

            for node_id, node in SYSTEM_REGISTRY.items():
                node_data = {"sensors": [], "actuators": []}
                for sensor in node.sensors.values():
                    status = sensor.get_status()
                    node_data["sensors"].append(status)
                    # CSVìš© ë¡œê·¸ ë°ì´í„° (1ë¶„ ë§ˆë‹¤)
                    # ì—¬ê¸°ì„œëŠ” 2ì´ˆë§ˆë‹¤ live_dataë¥¼ ë§Œë“¤ê³ , 60ì´ˆë§ˆë‹¤ CSVë¥¼ ê¸°ë¡í•˜ëŠ” ë¡œì§ì„ í†µí•©
                
                for act in node.actuators.values():
                    node_data["actuators"].append({
                        "id": act.device_id,
                        "name": act.name,
                        "state": act.state
                    })
                live_status[node_id] = node_data

            # 1. 2ì´ˆë§ˆë‹¤ ì‹¤ì‹œê°„ JSON ì—…ë°ì´íŠ¸ (ì›ìì  ì €ì¥: ì„ì‹œ íŒŒì¼ ì‚¬ìš© í›„ ì´ë¦„ ë³€ê²½)
            with open(live_data_path + ".tmp", 'w', encoding='utf-8') as f:
                json.dump({"timestamp": timestamp, "nodes": live_status}, f, ensure_ascii=False, indent=2)
            os.replace(live_data_path + ".tmp", live_data_path)

            # 2. 10ë¶„(600ì´ˆ)ë§ˆë‹¤ CSV ë° Google ì‹œíŠ¸ ëˆ„ì 
            if not hasattr(tsdb_logger_task, '_csv_counter'):
                tsdb_logger_task._csv_counter = 0
            
            tsdb_logger_task._csv_counter += 2 # 2ì´ˆ ì£¼ê¸°
            if tsdb_logger_task._csv_counter >= interval:
                tsdb_logger_task._csv_counter = 0
                
                # ì›”ë³„ íŒŒì¼ëª… ìƒì„± (ì˜ˆ: data/tsdb_2026_02.csv)
                now = datetime.now()
                monthly_file = f"{DATA_DIR}/tsdb_{now.strftime('%Y_%m')}.csv"
                
                # íŒŒì¼ì´ ì—†ìœ¼ë©´ í—¤ë” ìƒì„±
                if not os.path.exists(monthly_file):
                    os.makedirs(os.path.dirname(monthly_file), exist_ok=True)
                    with open(monthly_file, 'w', newline='', encoding='utf-8-sig') as f:
                        writer = csv.writer(f)
                        writer.writerow(["timestamp", "node_id", "device_id", "device_name", "value", "pin"])

                for node_id, node_data in live_status.items():
                    for s in node_data["sensors"]:
                        log_entries.append([timestamp, node_id, s['id'], s['name'], s['val'], s['pin']])
                
                if log_entries:
                    # A. ë¡œì»¬ CSV ì €ì¥
                    with open(monthly_file, 'a', newline='', encoding='utf-8-sig') as f:
                        writer = csv.writer(f)
                        writer.writerows(log_entries)
                    
                    # B. Google Sheets ì €ì¥ (ë¹„ë™ê¸°ë¡œ ì‹¤í–‰í•˜ê±°ë‚˜ ê°„ë‹¨íˆ ì²˜ë¦¬)
                    if GS_SHEET:
                        try:
                            GS_SHEET.append_rows(log_entries)
                            print(f"ï¿½ [Google] {len(log_entries)}ê±´ ì‹œíŠ¸ ì—…ë°ì´íŠ¸ ì™„ë£Œ.")
                        except Exception as e:
                            print(f"âš ï¸ [Google] ì‹œíŠ¸ ì“°ê¸° ì‹¤íŒ¨: {e}")
                            
                    print(f"ï¿½ğŸ“Š [TSDB] {timestamp} ì´ë ¥ ë°ì´í„° ì €ì¥ ì™„ë£Œ ({monthly_file})")
            
        except Exception as e:
            print(f"âš ï¸ [TSDB/Live Error] {e}")
        
        await asyncio.sleep(2) # ì‹¤ì‹œê°„ì„±ì„ ìœ„í•´ 2ì´ˆ ì£¼ê¸°ë¡œ ë³€ê²½

async def web_server_task():
    """
    ë¸Œë¼ìš°ì €ì˜ CORS ì •ì±…(file:// ì œí•œ)ì„ í”¼í•˜ê¸° ìœ„í•´
    í˜„ì¬ ë””ë ‰í† ë¦¬ë¥¼ ì›¹ ì„œë²„ë¡œ í˜¸ìŠ¤íŒ…í•©ë‹ˆë‹¤.
    ë˜í•œ /api/history ì—”ë“œí¬ì¸íŠ¸ë¥¼ í†µí•´ CSV ë°ì´í„°ë¥¼ JSONìœ¼ë¡œ ì œê³µí•©ë‹ˆë‹¤.
    """
    import http.server
    import socketserver
    import urllib.parse
    
    PORT = int(os.environ.get('PORT', 8000))

    class SmartFarmHandler(http.server.SimpleHTTPRequestHandler):
        def do_GET(self):
            # Health Check (Renderìš©)
            if self.path == '/health':
                self.send_response(200)
                self.end_headers()
                self.wfile.write(b"OK")
                return

            # ë£¨íŠ¸ ê²½ë¡œ(/) ë˜ëŠ” /index.html ì ‘ì† ì‹œ í™ë³´ í˜ì´ì§€(promo.html) ì¦‰ì‹œ ì„œë¹™
            # (ë§Œì•½ Dashboardë¥¼ ê°€ê³  ì‹¶ë‹¤ë©´ /html/index.html ë˜ëŠ” /dashboard.html ë“±ì„ í†µí•´ ì ‘ê·¼)
            parsed_path = urllib.parse.urlparse(self.path).path
            if parsed_path in ('/', '/index.html', '/index.htm'):
                promo_path = os.path.join(BASE_DIR, 'html', 'promo.html')
                if os.path.exists(promo_path):
                    self.send_response(200)
                    self.send_header('Content-type', 'text/html; charset=utf-8')
                    self.end_headers()
                    with open(promo_path, 'rb') as f:
                        self.wfile.write(f.read())
                else:
                    self.send_error(404, f"File Not Found: {promo_path}")
                return

            # API ìš”ì²­ ì²˜ë¦¬
            if self.path.startswith('/api/history'):
                self.handle_history_api()
            elif self.path.startswith('/api/journal'):
                self.handle_journal_list()
            elif self.path.startswith('/api/growth'):
                self.handle_growth_list()
            elif self.path.startswith('/api/run_model'):
                self.handle_run_model()
            else:
                # ê¸°ë³¸ ì •ì  íŒŒì¼ ì œê³µ
                super().do_GET()

        def translate_path(self, path):
            parsed_path = urllib.parse.urlparse(path).path
            file_name = parsed_path.lstrip('/')
            
            # 1. /data/ ìš”ì²­ì„ ì‹¤ì œ DATA_DIR í´ë”ë¡œ ë§¤í•‘ (ì ˆëŒ€ ê²½ë¡œ ë³´ì •)
            if parsed_path.startswith('/data/'):
                rel_path = parsed_path[len('/data/'):].lstrip('/')
                return os.path.join(BASE_DIR, DATA_DIR, rel_path)
            
            # 2. .html ìš”ì²­ì¸ ê²½ìš° /html/ í´ë” ë‚´ íŒŒì¼ì´ ìˆëŠ”ì§€ ìš°ì„  í™•ì¸
            if file_name.endswith('.html'):
                # ì´ë¯¸ 'html/' ê²½ë¡œê°€ í¬í•¨ë˜ì–´ ìˆë‹¤ë©´ ì¤‘ë³µ ë°©ì§€
                if file_name.startswith('html/'):
                    return os.path.join(BASE_DIR, file_name)
                
                # í¬í•¨ë˜ì–´ ìˆì§€ ì•Šë‹¤ë©´ 'html/' í´ë” ì•ˆì—ì„œ ê²€ìƒ‰
                html_path = os.path.join(BASE_DIR, 'html', file_name)
                if os.path.exists(html_path):
                    return html_path
                    
            # 3. ëª¨ë“  ì •ì  íŒŒì¼ ìš”ì²­ì„ BASE_DIR ê¸°ì¤€ìœ¼ë¡œ ë³€í™˜
            return os.path.join(BASE_DIR, file_name)

        def do_POST(self):
            # API ìš”ì²­ ì²˜ë¦¬ (POST)
            if self.path.startswith('/api/journal'):
                self.handle_journal_post()
            elif self.path.startswith('/api/analyze_growth'):
                self.handle_growth_analysis()
            elif self.path.startswith('/api/run_model'):
                self.handle_run_model()
            else:
                self.send_error(404, "Endpoint not found")

        def handle_run_model(self):
            """
            v2.5+: ì„œë²„ ë¶€í•˜ê°€ í° ì´ë¯¸ì§€ ìƒì„± ëŒ€ì‹ , ê³„ì‚°ëœ ë°ì´í„°(JSON)ë§Œ í´ë¼ì´ì–¸íŠ¸ì— ì „ë‹¬í•©ë‹ˆë‹¤.
            ê·¸ë˜í”„ ë“œë¡œì‰ì€ ë¸Œë¼ìš°ì €(Chart.js)ê°€ ë‹´ë‹¹í•©ë‹ˆë‹¤.
            """
            try:
                import growth_model
                # í˜„ì¬ ì„œë²„ê°€ ì‚¬ìš© ì¤‘ì¸ DATA_DIRë¥¼ í™˜ê²½ ë³€ìˆ˜ë¡œ ê°•ì œ ê³ ì •
                os.environ['DATA_DIR'] = DATA_DIR
                result = growth_model.run_analysis_data()
                
                self.send_response(200)
                self.send_header('Content-type', 'application/json')
                self.end_headers()
                self.wfile.write(json.dumps(result).encode('utf-8'))
                
            except Exception as e:
                print(f"âŒ [AI Model Error] {e}")
                # ì—ëŸ¬ ë°œìƒ ì‹œì—ë„ ë¸Œë¼ìš°ì €ê°€ 'H' ë¬¸ìë¥¼ ì½ì§€ ì•Šë„ë¡ JSONìœ¼ë¡œ ì‘ë‹µ
                self.send_response(500)
                self.send_header('Content-type', 'application/json')
                self.end_headers()
                self.wfile.write(json.dumps({"success": False, "error": str(e), "dates": []}).encode('utf-8'))

        def handle_growth_analysis(self):
            try:
                # 1. Body ì½ê¸° (Target Image URL)
                content_length = int(self.headers['Content-Length'])
                post_data = self.rfile.read(content_length)
                req_json = json.loads(post_data.decode('utf-8'))
                image_url = req_json.get('url', '')
                
                if not image_url:
                    self.send_error(400, "Image URL is missing")
                    return
                
                # 2. Vision Analysis ì‹¤í–‰
                if vision_analysis:
                    try:
                        result = vision_analysis.analyze_plant_growth(image_url)
                        
                        # [NEW] ë¶„ì„ ê²°ê³¼ íŒŒì¼ ì €ì¥
                        if result.get('success'):
                            log_file = f"{DATA_DIR}/growth_log.json"
                            logs = []
                            if os.path.exists(log_file):
                                try:
                                    with open(log_file, 'r', encoding='utf-8') as f:
                                        logs = json.load(f)
                                except: pass
                            
                            # Add log entry
                            logs.append({
                                "date": result['timestamp'],
                                "ratio": result['ratio'],
                                "pixels": result['green_pixels']
                            })
                            
                            with open(log_file, 'w', encoding='utf-8') as f:
                                json.dump(logs, f, indent=2)

                    except Exception as e:
                        result = {"error": f"Vision Engine Error: {str(e)}"}
                else:
                    result = {"error": "Vision Module Not Loaded. (Check terminal logs for import error)"}
                
                # 3. ê²°ê³¼ ë°˜í™˜
                self.send_response(200)
                self.send_header('Content-type', 'application/json')
                self.end_headers()
                self.wfile.write(json.dumps(result).encode('utf-8'))
                
            except Exception as e:
                print(f"Analysis API Error: {e}")
                self.send_error(500, str(e))

        def handle_journal_post(self):
            # ... (Existing Code) ...
            try:
                # 1. Body ì½ê¸°
                content_length = int(self.headers['Content-Length'])
                post_data = self.rfile.read(content_length)
                entry = json.loads(post_data.decode('utf-8'))
                
                # 2. íŒŒì¼ì— ì €ì¥ (prepend)
                file_path = f"{DATA_DIR}/journal.json"
                journals = []
                
                if os.path.exists(file_path):
                    with open(file_path, 'r', encoding='utf-8') as f:
                        try:
                            journals = json.load(f)
                        except json.JSONDecodeError:
                            journals = []
                
                journals.insert(0, entry) # ìµœì‹ ìˆœ
                
                with open(file_path, 'w', encoding='utf-8') as f:
                    json.dump(journals, f, ensure_ascii=False, indent=2)
                
                self.send_response(200)
                self.end_headers()
                self.wfile.write(b"OK")
                
            except Exception as e:
                print(f"Journal Save Error: {e}")
                self.send_error(500, str(e))


        def handle_history_api(self):
            try:
                # 1. íŒŒë¼ë¯¸í„° íŒŒì‹±
                query = urllib.parse.urlparse(self.path).query
                params = urllib.parse.parse_qs(query)
                target_date = params.get('date', [None])[0] # YYYY-MM-DD
                
                if not target_date:
                    self.send_error(400, "Missing 'date' parameter")
                    return

                # 2. ë°ì´í„° ìˆ˜ì§‘ (CSV + Google Sheets)
                ym_prefix = target_date[:7].replace('-', '_')
                file_path = f"{DATA_DIR}/tsdb_{ym_prefix}.csv"
                result_data = {"labels": [], "temp": [], "humi": []}
                
                # A. ë¡œì»¬ CSV ì‹œë„
                if os.path.exists(file_path):
                    encodings = ['utf-8-sig', 'utf-8']
                    lines = []
                    for enc in encodings:
                        try:
                            with open(file_path, 'r', encoding=enc) as f:
                                lines = f.readlines()
                            break
                        except: continue
                    if lines:
                        reader = csv.DictReader(lines)
                        for row in reader:
                            ts = row.get('timestamp', '')
                            if ts.startswith(target_date):
                                t_str = ts.split(' ')[1][:5]
                                dev = row.get('device_name', '')
                                try: val = float(row.get('value', '0'))
                                except: continue
                                if "ì˜¨ë„" in dev or "Temp" in dev:
                                    result_data["temp"].append({"t": t_str, "y": val})
                                elif "ìŠµë„" in dev or "Humi" in dev:
                                    result_data["humi"].append({"t": t_str, "y": val})

                # B. Google Sheets ë³´ì¶©
                if (not result_data["temp"] or not result_data["humi"]) and GS_SHEET:
                    print(f"ğŸŒ [API] Google Sheetsì—ì„œ {target_date} ë³µêµ¬ ì‹œë„...")
                    try:
                        all_rec = GS_SHEET.get_all_records()
                        for row in all_rec:
                            ts = str(row.get('timestamp', ''))
                            if ts.startswith(target_date):
                                t_str = ts.split(' ')[1][:5]
                                dev = row.get('device_name', '')
                                try: val = float(row.get('value', '0'))
                                except: continue
                                entry = {"t": t_str, "y": val}
                                if ("ì˜¨ë„" in dev or "Temp" in dev):
                                    if not any(x['t'] == t_str for x in result_data["temp"]):
                                        result_data["temp"].append(entry)
                                elif ("ìŠµë„" in dev or "Humi" in dev):
                                    if not any(x['t'] == t_str for x in result_data["humi"]):
                                        result_data["humi"].append(entry)
                        result_data["temp"].sort(key=lambda x: x["t"])
                        result_data["humi"].sort(key=lambda x: x["t"])
                    except Exception as ge: print(f"âš ï¸ [API] GS Error: {ge}")

                self.send_response(200)
                self.send_header('Content-type', 'application/json')
                self.end_headers()
                self.wfile.write(json.dumps(result_data).encode('utf-8'))
            except Exception as e:
                print(f"API Error: {e}")
                self.send_error(500, str(e))
        
        def handle_journal_list(self):
            try:
                file_path = f"{DATA_DIR}/journal.json"
                journals = []
                if os.path.exists(file_path):
                    with open(file_path, 'r', encoding='utf-8') as f:
                        try:
                            journals = json.load(f)
                        except: 
                            pass
                
                self.send_response(200)
                self.send_header('Content-type', 'application/json')
                self.end_headers()
                self.wfile.write(json.dumps(journals).encode('utf-8'))
            except Exception as e:
                self.send_error(500, str(e))
        
        def handle_growth_list(self):
            try:
                file_path = f"{DATA_DIR}/growth_log.json"
                logs = []
                if os.path.exists(file_path):
                    with open(file_path, 'r', encoding='utf-8') as f:
                        try:
                            logs = json.load(f)
                        except: pass
                
                self.send_response(200)
                self.send_header('Content-type', 'application/json')
                self.end_headers()
                self.wfile.write(json.dumps(logs).encode('utf-8'))
            except Exception as e:
                self.send_error(500, str(e))

    # í˜„ì¬ ë””ë ‰í† ë¦¬ë¥¼ ì„œë¹™í•˜ëŠ” í•¸ë“¤ëŸ¬ ìƒì„±
    
    server_started = False
    max_tries = 10
    retry_count = 0
    
    while retry_count < max_tries:
        try:
            # socketserver.TCPServerëŠ” ë¸”ë¡œí‚¹ì´ë¯€ë¡œ ìŠ¤ë ˆë“œì—ì„œ ì‹¤í–‰
            # íŒŒì´ì¬ 3.7+ ThreadingHTTPServer ê¶Œì¥ë˜ì§€ë§Œ í˜¸í™˜ì„± ìœ„í•´ TCPServer ì‚¬ìš©
            with socketserver.TCPServer(("0.0.0.0", PORT), SmartFarmHandler) as httpd:
                print(f"ğŸŒ [{DATA_DIR}] ì„œë²„ê°€ ê°€ë™ë˜ì—ˆìŠµë‹ˆë‹¤: http://0.0.0.0:{PORT}/")
                print(f"   ã„´ API ì—”ë“œí¬ì¸íŠ¸: http://localhost:{PORT}/api/history")
                server_started = True
                await asyncio.to_thread(httpd.serve_forever)
                break
        except OSError as e:
            if 'PORT' in os.environ:
                # Renderì™€ ê°™ì´ í™˜ê²½ ë³€ìˆ˜ë¡œ í¬íŠ¸ê°€ ì§€ì •ëœ ê²½ìš°, í•´ë‹¹ í¬íŠ¸ê°€ ì•ˆ ë˜ë©´ ì¦‰ì‹œ ì—ëŸ¬
                print(f"âŒ ì§€ì •ëœ í¬íŠ¸ {PORT}ë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {e}")
                raise
            print(f"âš ï¸ í¬íŠ¸ {PORT}ê°€ ì´ë¯¸ ì‚¬ìš© ì¤‘ì…ë‹ˆë‹¤. ë‹¤ìŒ í¬íŠ¸ë¡œ ì‹œë„í•©ë‹ˆë‹¤...")
            PORT += 1
            retry_count += 1
    
    if not server_started:
        print("âŒ ì›¹ ì„œë²„ë¥¼ ì‹œì‘í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

async def dynamic_coordinator_task():
    """
    í•˜ë£¨ 4ë²ˆ(00, 06, 12, 18ì‹œ) ë‚ ì§œë¥¼ ì ê²€í•˜ì—¬ êµ¬ì—­ë³„ ì¬ë°° ë‹¨ê³„ ë° ì„ê³„ê°’ì„ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤.
    ì´ˆê¸° ì‹¤í–‰ ì‹œ 1íšŒ ì¦‰ì‹œ ë™ê¸°í™”ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤.
    """
    CHECK_HOURS = {0, 6, 12, 18}
    print(f"ğŸ“… [Coordinator] ì •ê¸° ì—…ë°ì´íŠ¸ ëª¨ë“œ ê°€ë™ (ì˜ˆì • ì‹œê°„: {sorted(list(CHECK_HOURS))}ì‹œ)")
    
    last_run_hour = -1
    last_processed_stages = {} # {node_id: last_recipe}
    first_run = True

    while True:
        try:
            now = datetime.now()
            # ì •í•´ì§„ ì‹œê°„ì´ê±°ë‚˜ ì´ˆê¸° ì‹¤í–‰ì¸ ê²½ìš°
            if first_run or (now.hour in CHECK_HOURS and now.hour != last_run_hour):
                # 1. ì„¤ì • ë¡œë“œ
                with open(f'{DATA_DIR}/zone_config.json', 'r', encoding='utf-8') as f:
                    zones = json.load(f)
                
                for zone in zones:
                    zone_id_prefix = zone['id']
                    crop = zone.get('crop', 'none')
                    schedule = zone.get('schedule', {})
                    
                    # 2. í˜„ì¬ ë‚ ì§œì— ë”°ë¥¸ ì¬ë°° ë‹¨ê³„ ê²°ì •
                    current_stage = "sowing"
                    sorted_stages = sorted(
                        [(k, datetime.strptime(v, "%Y-%m-%d")) for k, v in schedule.items()],
                        key=lambda x: x[1], reverse=True
                    )
                    for stage, date in sorted_stages:
                        if now >= date:
                            current_stage = stage
                            break
                    
                    target_recipe = f"{crop}.{current_stage}"
                    
                    # 3. í•´ë‹¹ êµ¬ì—­ì˜ ë…¸ë“œë“¤ì„ ì°¾ì•„ ì„ê³„ê°’ ì—…ë°ì´íŠ¸
                    for node_id, node in SYSTEM_REGISTRY.items():
                        if node_id.startswith(zone_id_prefix):
                            if last_processed_stages.get(node_id) != target_recipe:
                                success = node.update_thresholds(target_recipe)
                                if success:
                                    prefix = "ğŸš€ [Initial]" if first_run else f"â° [{now.hour:02d}:00]"
                                    print(f"{prefix} {node_id} ë‹¨ê³„ í™•ì¸: {target_recipe} ì„ê³„ê°’ ì ìš©")
                                    last_processed_stages[node_id] = target_recipe

                last_run_hour = now.hour
                first_run = False

        except Exception as e:
            print(f"âš ï¸ [Coordinator Error] {e}")
        
        # 1ë¶„ ë‹¨ìœ„ë¡œ ì²´í¬
        await asyncio.sleep(60)

async def main():
    # 0. Google Sheets ì´ˆê¸°í™” (ì´ë²¤íŠ¸ ë£¨í”„ ì‹œì‘ í›„ ìˆ˜í–‰)
    init_google_sheets()

    # 1. íŒŒì¼ì—ì„œ ì„¤ì • ë¡œë“œ
    try:
        with open(f'{DATA_DIR}/config.json', 'r', encoding='utf-8') as f:
            config_data = json.load(f)
    except FileNotFoundError:
        print(f"{DATA_DIR}/config.json íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ì–´ ê¸°ë³¸ ì‹œë®¬ë ˆì´ì…˜ì„ ì‹¤í–‰í•©ë‹ˆë‹¤.")
        return

    all_tasks = []
    print(f"[{len(config_data)}ê°œì˜ ë…¸ë“œ ì„¤ì • ë¡œë“œ ì™„ë£Œ...]")

    for node_cfg in config_data:
        node_id = node_cfg['id']
        node = ESP32C3Node(node_id)
        node.provision(node_cfg)
        
        # í• ë‹¹ëœ í•€ ì •ë³´ ì¶œë ¥
        print(f"   [{node_id}] Pin Map: ", end="")
        pin_info = [f"{dev_id}({info['pin']})" for dev_id, info in node.get_pin_map().items()]
        print(", ".join(pin_info))
        
        # ë¹„ë™ê¸° ì‹¤í–‰ ì¶”ê°€
        interval = random.uniform(4, 6)
        all_tasks.append(node.run_forever(interval=interval))

    # 2. íƒœìŠ¤í¬ ì¶”ê°€ (5ë¶„=300ì´ˆ ê°„ê²©ìœ¼ë¡œ ë¡œê·¸ ê¸°ë¡)
    all_tasks.append(tsdb_logger_task(interval=300))
    all_tasks.append(web_server_task())
    all_tasks.append(dynamic_coordinator_task())

    print(f"\n[ì‹¤í–‰ ì‹œì‘] ëª¨ë“  ë…¸ë“œì™€ í†µí•© ì„œë²„ê°€ ì‘ë™í•©ë‹ˆë‹¤.")
    print("------------------------------------------------------------------")

    try:
        # ëª¨ë“  íƒœìŠ¤í¬ê°€ ì¢…ë£Œë  ë•Œê¹Œì§€ ë¬´í•œ ì‹¤í–‰
        await asyncio.gather(*all_tasks)
    except KeyboardInterrupt:
        print("\n[ì •ì§€] ì‚¬ìš©ìê°€ í”„ë¡œê·¸ë¨ì„ ì¢…ë£Œí–ˆìŠµë‹ˆë‹¤.")
    except Exception as e:
        print(f"\n[ì˜¤ë¥˜ ë°œìƒ] {e}")

if __name__ == "__main__":
    asyncio.run(main())
